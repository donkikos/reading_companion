{
  "project": "Epub AI Reader",
  "branchName": "ralph/ingestion-v2",
  "description": "V2 Ingestion (Qdrant Chunk Indexing) - index EPUBs into chunked Qdrant payloads for spoiler-safe retrieval.",
  "userStories": [
    {
      "id": "US-001",
      "title": "Build deterministic sentence stream",
      "description": "As a reader, I want my uploaded book parsed into a stable sentence stream so indexing is consistent.",
      "acceptanceCriteria": [
        "Parse the EPUB into an ordered list of sentences S0..SN.",
        "Sentence IDs are monotonic and contiguous for each book.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-002",
      "title": "Create fixed-window chunks",
      "description": "As a reader, I want my book chunked consistently so retrieval quality is reliable.",
      "acceptanceCriteria": [
        "Create chunks using a fixed window of 8 sentences with overlap of 2.",
        "Each chunk covers the ordered sentence-id interval `[pos_start..pos_end]`.",
        "For each chunk, `sentences[k]` maps to `sid = pos_start + k`.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-003",
      "title": "Store chunk payloads in Qdrant",
      "description": "As a reader, I want my uploaded book indexed in Qdrant so spoiler-safe retrieval can use it.",
      "acceptanceCriteria": [
        "Store payload fields: `book_id`, `chapter_index`, `pos_start`, `pos_end`, `sentences`, `text`.",
        "Chunk embeddings are computed from the full chunk `text`.",
        "Qdrant points are written for each chunk of the current book.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-004",
      "title": "Replace chunks on re-upload",
      "description": "As a reader, I want re-uploading a book to replace old indexes so answers stay consistent.",
      "acceptanceCriteria": [
        "Re-ingestion deletes or replaces all prior chunks for the same `book_id`.",
        "No duplicate chunks remain after re-ingest.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-005",
      "title": "Track ingestion progress",
      "description": "As a reader, I want to see ingestion progress so I know when the book is ready.",
      "acceptanceCriteria": [
        "Progress is tracked as a best-effort percentage during ingestion.",
        "Progress is based on sentences processed out of total.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-006",
      "title": "Fail clearly if Qdrant is unavailable",
      "description": "As a reader, I want a clear failure when indexing cannot run so I can retry later.",
      "acceptanceCriteria": [
        "Ingestion fails with an explicit error when Qdrant cannot be reached.",
        "Partial ingestion is not marked as successful.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-007",
      "title": "Verify ingestion via API",
      "description": "As a developer, I want an API endpoint to verify Qdrant ingestion so I can confirm indexing without relying on retrieval.",
      "acceptanceCriteria": [
        "Provide an API endpoint that accepts `book_id` and verifies Qdrant ingestion for that book.",
        "The endpoint recomputes expected chunk count from the sentence stream and chunking params.",
        "The endpoint queries Qdrant by `book_id` and confirms the stored count matches expected.",
        "The endpoint validates payload fields and `pos_start`/`pos_end` monotonicity on a sample of chunks.",
        "The endpoint returns a clear pass/fail response with details on mismatches.",
        "If Qdrant is unavailable, the endpoint returns an explicit error.",
        "Restart the app server after implementing this story before verifying the endpoint.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-008",
      "title": "Single command to launch app + Qdrant",
      "description": "As a developer, I want a single `docker compose` command to start the app and Qdrant so ingestion can run without manual service setup.",
      "acceptanceCriteria": [
        "Provide one `docker compose` command that starts the FastAPI app and Qdrant.",
        "The command performs a health check that confirms the app responds and Qdrant is reachable.",
        "The command exits cleanly and stops both services.",
        "Restart the app server after implementing this story before running health checks.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-009",
      "title": "Add EPUB fixture for automated ingestion tests",
      "description": "As a developer, I want a stable EPUB fixture so ingestion tests can run deterministically.",
      "acceptanceCriteria": [
        "Add a fixture EPUB at `tests/fixtures/minimal.epub` with multiple chapters and paragraphs.",
        "Tests can ingest the fixture and verify Qdrant chunk counts using the verification API.",
        "Restart the app server after implementing this story before running the ingestion tests.",
        "The fixture stays ASCII-only and stable across runs.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-010",
      "title": "Delete a book with confirmation",
      "description": "As a reader, I want to delete a book with confirmation so I can remove it safely.",
      "acceptanceCriteria": [
        "Provide a delete action in the library UI that opens a confirmation dialog.",
        "Confirming delete removes the book file and all stored data (SQLite metadata, reading state, chapters, Chroma docs, Qdrant chunks).",
        "Cancelling delete leaves the book untouched.",
        "The library UI updates to remove the deleted book without a full reload.",
        "Restart the app server after implementing this story before verifying the UI.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes",
        "Verify in browser using MCP"
      ],
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-011",
      "title": "End-to-end ingestion progress in the UI",
      "description": "As a reader, I want clear progress updates across all ingestion stages so I know what is happening.",
      "acceptanceCriteria": [
        "Uploading a book shows progress messages and percentage updates in the library UI.",
        "The UI surfaces stage transitions (hashing, parsing, chunking, embedding, Qdrant upsert, metadata save).",
        "The progress percentage reflects the full ingestion pipeline, not only sentence streaming.",
        "Sentence-based progress may be tracked internally, but the UI must reflect the full pipeline only.",
        "The UI shows top-level stage progress as `i/N` with an overall progress number.",
        "Lower-level percent is optional (e.g., sentence parsing) when reasonable.",
        "The task status reaches \"completed\" only after all storage steps finish.",
        "Errors during ingestion surface a clear error message in the UI.",
        "If any ingestion error occurs during verification, the story fails verification.",
        "Restart the app server after implementing this story before verifying the UI.",
        "Verify in browser using MCP.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "high"
    },
    {
      "id": "US-012",
      "title": "Configure embedding provider for ingestion",
      "description": "As a developer, I want ingestion to use a configurable Ollama embedding service so we can improve retrieval quality without changing APIs.",
      "acceptanceCriteria": [
        "Embeddings are generated by calling Ollama `/api/embed` during ingestion.",
        "The embedding model name and base URL are configurable (default model: `BAAI/bge-base-en-v1.5`).",
        "Optional embedding dimensions are configurable (when supported by Ollama).",
        "If the embedding service is unavailable, ingestion fails with a clear error.",
        "Ollama runs as a docker compose service and uses a volume for model storage.",
        "Add local Ollama model cache paths to `.gitignore`.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "high"
    },
    {
      "id": "US-013",
      "title": "Persist embedding metadata",
      "description": "As a developer, I want ingestion to record embedding metadata so retrieval can detect mismatches.",
      "acceptanceCriteria": [
        "The ingestion pipeline records the embedding model name and dimensions used.",
        "The ingestion pipeline documents that model changes require reindexing.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "medium"
    },
    {
      "id": "US-014",
      "title": "Use Ollama embeddings for Qdrant vectors",
      "description": "As a reader, I want Qdrant vectors to reflect semantic embeddings so retrieval quality is meaningful.",
      "acceptanceCriteria": [
        "Qdrant vectors are generated from Ollama embeddings of the full chunk text (not hash placeholders).",
        "Run an end-to-end ingestion using a live Ollama model and confirm embeddings are stored in Qdrant.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "high"
    },
    {
      "id": "US-015",
      "title": "Remove Chroma ingestion",
      "description": "As a developer, I want to remove Chroma writes so ingestion only targets Qdrant and SQLite.",
      "acceptanceCriteria": [
        "Ingestion no longer writes documents or metadata to Chroma.",
        "All Chroma-related code paths are removed.",
        "Delete flow does not attempt Chroma cleanup once Chroma is removed.",
        "`/sync` uses Qdrant (not Chroma) for matching and position updates.",
        "Removal is documented and tests are updated accordingly.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": false,
      "notes": "",
      "priority": "low"
    },
    {
      "id": "US-016",
      "title": "Log ingestion timing metrics",
      "description": "As a developer, I want ingestion timing metrics so we can validate performance on CPU-only Macs.",
      "acceptanceCriteria": [
        "Logs are structured JSON at debug level.",
        "Logs include total ingestion time.",
        "Logs include time spent generating embeddings.",
        "Logs include time spent upserting to Qdrant.",
        "Logs include chunks processed and chunks/sec.",
        "Logs are emitted to stdout.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": false,
      "notes": "",
      "priority": "low"
    },
    {
      "id": "US-017",
      "title": "Migrate embeddings away from Ollama to TEI (High Priority)",
      "description": "As a developer, I want ingestion embeddings served by Hugging Face Text Embeddings Inference (TEI) so the configured model is actually served and reliable without Ollama.",
      "acceptanceCriteria": [
        "Replace the Ollama embedding dependency with a TEI service in Docker Compose.",
        "Remove Ollama completely (compose service, env vars, code paths, docs/tests).",
        "Use the CPU image `ghcr.io/huggingface/text-embeddings-inference:cpu-1.8.1` on macOS.",
        "Switching to GPU is done by changing the Docker Compose image tag (no code changes).",
        "TEI is configured to serve the target embedding model.",
        "Ingestion embeddings are generated via TEI and succeed for a real EPUB upload.",
        "TEI requests use the `/embed` endpoint and return an array of float arrays.",
        "The embedding base URL and model name are configurable via environment variables (default model: `bge-base-en-v1.5`).",
        "The model uses fixed dimensions; the configured Qdrant vector size matches the TEI embedding dimension.",
        "The model is cached and reused across restarts (TEI model cache volume).",
        "The app surfaces a clear error if the embedding service is unavailable.",
        "Document in `README` the TEI image tag, how to switch to GPU via image tag, and required env vars (model name, base URL, batching).",
        "Note that this story supersedes Ollama-specific stories (US-012/US-014) without changing their historical status in `prd.json`.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "high"
    },
    {
      "id": "US-018",
      "title": "Chunking within chapter boundaries",
      "description": "As a reader, I want chunking to stay within chapter boundaries so context is consistent.",
      "acceptanceCriteria": [
        "Chunks are created per-chapter using the same window/overlap params.",
        "Chunks do not cross chapter boundaries.",
        "The last partial chunk in each chapter is kept.",
        "`chapter_index` always matches the chunk's chapter.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "medium"
    },
    {
      "id": "US-019",
      "title": "Data consistency check on server launch",
      "description": "As a developer, I want server launch to clean up orphaned Qdrant data so storage stays consistent.",
      "acceptanceCriteria": [
        "On startup, scan local books in SQLite and remove Qdrant chunks for missing book IDs.",
        "Cleanup runs before ingestion requests are accepted.",
        "If Qdrant is unavailable, the server fails startup with a clear error.",
        "Actions and errors are logged clearly.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "high"
    },
    {
      "id": "US-020",
      "title": "Purge old ingested Qdrant data after TEI migration",
      "description": "As a developer, I want Qdrant data purged after TEI migration so all embeddings are consistent.",
      "acceptanceCriteria": [
        "At the start of implementing this story, purge all existing Qdrant chunks via API calls or CLI commands.",
        "Cleanup is logged clearly and is idempotent.",
        "Run `ruff format` on changed Python files (line length 100)",
        "Run `ruff check .` and ensure it passes",
        "Add or update tests for this change",
        "Tests pass",
        "Run `pytest` and ensure it passes",
        "Typecheck/lint passes"
      ],
      "passes": true,
      "notes": "",
      "priority": "medium"
    }
  ]
}
