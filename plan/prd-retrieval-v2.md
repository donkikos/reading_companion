# PRD: V2 Retrieval (Spoiler-Safe Qdrant Query)

## Introduction/Overview

Implement V2 retrieval that queries Qdrant for chunked content and enforces strict spoiler safety by truncating output at the user's reading position.

## Goals

- Retrieve relevant chunks using dense vector search with a spoiler-safe filter.
- Truncate returned content so no sentence past the user position is exposed.
- Keep retrieval deterministic and safe without reranking or hybrid search.
- Use Ollama embeddings as a configurable external dependency.
- Run Ollama as a docker compose service with persistent model storage.

## User Stories

### US-001: Configure the embedding provider (Ollama)

**Description:** As a developer, I want embeddings generated by a configurable Ollama service so retrieval quality can improve without changing APIs.

**Acceptance Criteria:**

- [ ] Embeddings are generated by calling Ollama `/api/embed`.
- [ ] The embedding model name and base URL are configurable (default model: `BAAI/bge-base-en-v1.5`).
- [ ] Optional embedding dimensions are configurable (when supported by Ollama).
- [ ] If the embedding service is unavailable, the retrieval request fails with a clear error.
- [ ] Ollama runs as a docker compose service and uses a volume for model storage.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-002: Generate query embeddings

**Description:** As a reader, I want my query embedded via Ollama so Qdrant returns semantically relevant chunks.

**Acceptance Criteria:**

- [ ] Retrieval generates a query embedding using Ollama `/api/embed` with the query string.
- [ ] The embedding model matches the model used during ingestion.
- [ ] Retrieval errors clearly if the embedding model/dimension metadata does not match ingestion.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-003: Query Qdrant for the current book

**Description:** As a reader, I want the assistant to query Qdrant for my current book so answers use the right context.

**Acceptance Criteria:**

- [ ] When I ask a question, the MCP retrieves context from Qdrant for the current `book_id`.
- [ ] The MCP reads `user_pos` from the persisted reading state before querying Qdrant.
- [ ] Qdrant queries include a filter `pos_start <= user_pos`.
- [ ] Results are restricted to the current `book_id`.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-004: Truncate chunks at the spoiler boundary

**Description:** As a reader, I want answers to never include sentences beyond my reading position.

**Acceptance Criteria:**

- [ ] For each chunk, iterate `sentences` and compute `sid = pos_start + k`.
- [ ] Stop including sentences when `sid > user_pos`.
- [ ] Rejoin only safe sentences for final context.
- [ ] Verify that a partially read chunk only returns sentences at or before `user_pos`.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-005: Deduplicate overlapping context

**Description:** As a reader, I want retrieved context without repeated sentences so answers are clear.

**Acceptance Criteria:**

- [ ] Overlapping sentences across chunks are removed from the final context.
- [ ] The final context preserves original sentence order.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-006: Return top-k relevant chunks

**Description:** As a reader, I want responses limited to the most relevant chunks so answers are efficient and focused.

**Acceptance Criteria:**

- [ ] Default retrieval returns top-k 20 chunks.
- [ ] Queries use dense vector search without reranking.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

## Functional Requirements

- FR-1: Retrieval must filter Qdrant results with `pos_start <= user_pos`.
- FR-2: Retrieval must filter by `book_id`.
- FR-3: Returned context must be truncated at `user_pos` using sentence IDs.
- FR-4: Overlapping sentences across chunks must be deduplicated.
- FR-5: Default retrieval returns top-k 20 chunks.
- FR-6: Retrieval is exposed through the MCP server for LLM access.
- FR-7: The MCP must read `user_pos` from the persisted reading state before retrieval.
- FR-8: Retrieval must generate query embeddings via Ollama `/api/embed`.
- FR-9: Embedding model and Ollama base URL must be configurable.
- FR-10: Retrieval must fail with a clear error if the embedding service is unavailable.
- FR-11: Retrieval must use the same embedding model as ingestion.
- FR-12: Ollama runs as a docker compose service with persistent model storage.

## Non-Goals (Out of Scope)

- Reranking or cross-encoder scoring.
- Sparse or hybrid retrieval.
- Query-less retrieval modes.

## Design Considerations

- Truncation must happen after retrieval and before the LLM receives any text.
- Deduplication should preserve sentence order from the global stream.

## Technical Considerations

- Qdrant filter should use payload indexes for `book_id` and `pos_start`.
- Retrieval should accept a query string; a query is required.
- MCP must obtain `user_pos` from the persisted reading state before querying Qdrant.
- Ollama embeddings use `POST /api/embed` with `input` as a string; response returns `embeddings`.
- Embedding dimensionality must match the Qdrant collection configuration; changing models requires reindexing.

## Success Metrics

- Queries never return any sentence with `sid > user_pos`.
- Retrieval completes within an acceptable latency for local use.

## Open Questions

None.
