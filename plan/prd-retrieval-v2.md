# PRD: V2 Retrieval (Spoiler-Safe Qdrant Query)

## Introduction/Overview

Implement V2 retrieval that queries Qdrant for chunked content and enforces strict spoiler safety by truncating output at the user's reading position.

## Goals

- Retrieve relevant chunks using dense vector search with a spoiler-safe filter.
- Truncate returned content so no sentence past the user position is exposed.
- Keep retrieval deterministic and safe without reranking or hybrid search.
- Use TEI embeddings as a configurable external dependency.
- Run TEI as a docker compose service with persistent model storage.

## User Stories

### US-001: Configure the embedding provider (TEI)

**Description:** As a developer, I want embeddings generated by a configurable TEI service so retrieval quality can improve without changing APIs.

**Acceptance Criteria:**

- [ ] Embeddings are generated by calling TEI `/embed`.
- [ ] The embedding model name and base URL are configurable (defaults: `TEI_MODEL=BAAI/bge-base-en-v1.5`, `TEI_BASE_URL=http://tei:80` in docker compose).
- [ ] TEI batch size and timeout are configurable (`TEI_BATCH_SIZE` default 8, `TEI_TIMEOUT` default 30).
- [ ] If the embedding service is unavailable, the retrieval request fails with a clear error (HTTP 503).
- [ ] TEI runs as a docker compose service and uses a volume for model storage.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-002: Generate query embeddings

**Description:** As a reader, I want my query embedded via TEI so Qdrant returns semantically relevant chunks.

**Acceptance Criteria:**

- [ ] Retrieval generates a query embedding using TEI `/embed` with the query string.
- [ ] The embedding model matches the model used during ingestion.
- [ ] Reject empty or whitespace-only queries with a clear error message (HTTP 400).
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-003: Query Qdrant for the current book

**Description:** As a reader, I want the assistant to query Qdrant for my current book so answers use the right context.

**Acceptance Criteria:**

- [ ] When I ask a question, the MCP retrieves context from Qdrant for the current `book_id`.
- [ ] The MCP reads `user_pos` from the persisted reading state before querying Qdrant.
- [ ] Qdrant queries include a spoiler-safe filter (`pos_end <= user_pos`, inclusive).
- [ ] Results are restricted to the current `book_id`.
- [ ] MCP provides a `list_books` tool that returns structured JSON with `book_id`, `title`, and `author` so the agent can choose the correct `book_id`.
- [ ] `list_books` response schema:
      ```json
      {"books":[{"book_id":"string","title":"string","author":"string"}]}
      ```
- [ ] `book_id` is the canonical book identifier (same value used in Qdrant payloads and SQLite book hash).
- [ ] `user_pos = 0` means the book was just added and the user has not started reading.
- [ ] If reading state is missing or `user_pos` is null/invalid, retrieval fails with a clear error (HTTP 400).
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-004: Truncate chunks at the spoiler boundary (sentence-based context)

**Description:** As a reader, I want answers to never include sentences beyond my reading position.

**Acceptance Criteria:**

- [ ] When assembling sentence-based context, iterate `sentences` and compute `sid = pos_start + k` (k is zero-based; sentence IDs start at 0).
- [ ] Stop including sentences when `sid > user_pos` (include `sid == user_pos`).
- [ ] Rejoin only safe sentences for final context.
- [ ] Semantic search results are restricted via the spoiler-safe Qdrant filter (no extra truncation needed).
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-005: Deduplicate overlapping context

**Description:** As a reader, I want sentence-based context without repeated sentences so answers are clear.

**Acceptance Criteria:**

- [ ] Overlapping sentences across chunks are removed from sentence-based context (dedupe by `sid`).
- [ ] The final context preserves original sentence order.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

### US-006: Return top-k relevant chunks

**Description:** As a reader, I want responses limited to the most relevant chunks so answers are efficient and focused.

**Acceptance Criteria:**

- [ ] Default retrieval returns top-k 20 chunks.
- [ ] The MCP retrieval tool accepts a `k` argument (max 256); if omitted, default to 20.
- [ ] `k` controls the number of semantic-search chunks returned when a query is provided; when no query is provided, `k` controls the number of sentences returned.
- [ ] Queries use dense vector search without reranking.
- [ ] Run `ruff format` on changed Python files (line length 100)
- [ ] Run `ruff check .` and ensure it passes
- [ ] Add or update tests for this change
- [ ] Tests pass
- [ ] Run `pytest` and ensure it passes
- [ ] Typecheck/lint passes

## Functional Requirements

- FR-1: Retrieval must filter Qdrant results with a spoiler-safe constraint (currently `pos_end <= user_pos`).
- FR-2: Retrieval must filter by `book_id`.
- FR-3: Sentence-based context must be truncated at `user_pos` using sentence IDs.
- FR-4: Overlapping sentences across chunks must be deduplicated for sentence-based context.
- FR-5: Default retrieval returns top-k 20 chunks.
- FR-6: Retrieval is exposed through the MCP server for LLM access.
- FR-7: The MCP must read `user_pos` from the persisted reading state before retrieval.
- FR-8: Retrieval must generate query embeddings via TEI `/embed`.
- FR-9: Embedding model and TEI base URL must be configurable.
- FR-10: Retrieval must fail with a clear error if the embedding service is unavailable.
- FR-11: Retrieval must use the same embedding model as ingestion.
- FR-12: TEI runs as a docker compose service with persistent model storage.

## Non-Goals (Out of Scope)

- Reranking or cross-encoder scoring.
- Sparse or hybrid retrieval.
- Query-less retrieval modes.

## Design Considerations

- Truncation must happen after retrieval and before the LLM receives any text.
- Deduplication should preserve sentence order from the global stream.

## Technical Considerations

- Qdrant filter should use payload indexes for `book_id` and `pos_end`.
- Retrieval should accept a query string; a query is required.
- MCP must obtain `user_pos` from the persisted reading state before querying Qdrant.
- TEI embeddings use `POST /embed` with `inputs` as a list of strings; response returns a list of float arrays.
- Embedding dimensionality must match the Qdrant collection configuration; changing models requires reindexing.

## Success Metrics

- Queries never return any sentence with `sid > user_pos`.
- Retrieval completes within an acceptable latency for local use.

## Open Questions

None.
